# -*- coding: utf-8 -*-
"""Sentiment Analysis-Netflix Reviews-NLP-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hsKhamRL3uxhZbmlhMN3beCwZfhrmBzz

##Importing Libraries
"""

pip install contractions

import nltk
nltk.download('stopwords')

import pandas as pd
import numpy as np
import nltk
import string
import matplotlib.pyplot as plt
import seaborn as sns
import re
from nltk.corpus import stopwords
import time
from nltk.stem import PorterStemmer

import warnings
warnings.filterwarnings('ignore')

"""##IMPORTING DATASETS"""

df=pd.read_csv(r'/content/drive/MyDrive/PROJECTS DS/Sentimental Analysis/netflix_reviews.csv')

df.head()

"""##Data Exploration and Cleaning"""

df.shape

df.info()

df.isna().sum()

df.duplicated().sum()

df.describe()

"""# Dataset Summary
This dataset contains information about the reviews given by netflix users on Google Play Store. Apart from the reviews, it also contains information about the ratings and the date of review as well as the likes on each of the review. There are total of 112271 reviews provided for the Netflix application.

##*Column Description*



*   LreviewId: represents a unique review id corresponding to each review
*   userName: provides information about the name of user who has given the review. There are total 79291 unique usernames which provided the review.


* content: it is the review provided by user

* score: it is the rating value provided along-side the review indicating the level of satisfaction. It has 5 categories
* thumbsUpCount: it is the total likes received by a particular review by other users


*  reviewCreatedVersion: it is version of review given by the user

##*Data Cleaning*
Issues with the dataset


*  reviewId, userName, reviewCreatedVersion & appVersion are irrelevant to the project, so dropping those.
*  There are 2 reviews missing from the dataset
* There are 287 duplicates in the dataset.
*There are 1 username missing.There are 16281 app & review version missing from the dataset. Both of them will be handled when those features are removed.
*only 25% reviews have 1 or more thumbs up on their review. It is very skewed.so dropping those as well
*year can be extracted for better analysis
*Instead of scores from 1 to 5, i am converting them to emotions to predict sentiments like
      Positive
      Neutral
      Negative






"""

#Removing unwanted columns

df.columns

df.drop(['reviewId', 'userName', 'thumbsUpCount',
       'reviewCreatedVersion', 'appVersion'],axis=1,inplace=True)

df.head()

#removing null values

df.isna().sum()

df.dropna(inplace=True)

df.isna().sum()

#extracting the year to plot the relation between reviews and years

df['year']=pd.to_datetime(df['at']).apply(lambda x:x.year).astype(int)
df.drop('at',axis=1,inplace=True)
df.head()

#renaming content and score to review and sentiment

df=df.rename(mapper={'content':'review','score':'sentiment'},axis=1)

df.head()

"""## Text Preprocessing"""

#emoji removal

import re

def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002700-\U000027BF"  # dingbats
        "\U0001F900-\U0001F9FF"  # supplemental symbols and pictographs
        "\U00002600-\U000026FF"  # miscellaneous symbols
        "\U00002B50-\U00002B59"  # stars
        "\U0001FA70-\U0001FAFF"  # symbols and pictographs extended-A
        "\U00002500-\U00002BEF"  # Chinese symbols
        "\U0001F004"             # Mahjong tile red dragon
        "\U0001F0CF"             # Playing card black joker
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', text)

import contractions

ps=PorterStemmer()

def clean_text(text):
    # remove numbers
    text = ''.join([i for i in text if not i.isdigit()])
    # convert to lower case
    text = text.lower()
    # expand contractions
    text = contractions.fix(text)
    # remove punctuations
    text = ''.join([character for character in text if character not in string.punctuation])

    # remove stop words
    text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])
    # perform stemming
    text = text.split()
    words = []
    for word in text:
        words.append(ps.stem(word))
    text = ' '.join(words)
    # remove emoji
    text = remove_emojis(text)
    # remove extra spaces
    text = ' '.join(text.split())
    return text

start_time = time.time()
df['review'] = df['review'].apply(clean_text)
print(time.time()-start_time)

#adding a feature to the dataset to count the characters of the string to understand their relation

df['characters']=df['review'].apply(lambda x:len(''.join(x.split())))
df.head()

#changing the sentiment column to show sentiments like positive(0),neutral(1) and negative(2).
#logic-
# 4 or 5 -Positive---0
#   3    -Neutral----1
#  <3    -Negative---2

def sentiment_class(score):
  if score>=4:
    return 0
  elif score==3:
    return 1
  else:
    return 2

df['sentiment']=df['sentiment'].apply(sentiment_class)

df.head()

"""##Plotting some relations"""

plt.figure(figsize=(12,5))
sns.countplot(data=df,x='sentiment',color='grey')
plt.suptitle('Sentiment Distribution')
plt.show()

"""It can be seen that the sentiment is distributed almost evenly among positive and negative reviews.Neutral is unbalanced.Thus this dataset will be able to predict positive and negative reviews more accurately than neutral reviews."""

plt.figure(figsize=(12,5))
sns.boxplot(data=df,x='sentiment',y='characters',palette='rainbow')
plt.suptitle('Review total characters vs sentiment')
plt.show()

"""It can be seen that total characters of the review is between 0 and 200 for all the sentiments and there is a lot of outliers as well.so characters may not be the best fitted for classification"""

plt.figure(figsize=(14,6))
sns.set_style('whitegrid')
sns.countplot(data=df,x='year',hue='sentiment',palette={0:'green',1:'pink',2:'red'})
plt.suptitle('Yearly Sentiment Trend')
plt.legend(labels=['Neutral','Negative','Positive'])
plt.show()

"""## Chart Inference

From this graph it can be seen
 * from 2018 to 2020 the positive reviews was greater than negative reviews
 * From 2021 to 2024 the negative reviews are more than positive reviews.
 * An event may have happened in 2020 for this trend (further studies can be made on what happened)----may be because of the increase in users due to covid crisis

"""

#Now we can drop characters as they are not really contributing to the sentiment and also as we are only analyzing
#the review at the end to predict sentiment lets drop the year as well.

df.drop(['characters','year'],axis=1,inplace=True)

df.head()

"""## Splitting the Data"""

#reducing the sample size due to large time in processing the whole data for multiple models (can be applied to the whole data)
df=df.sample(30000)

X=df['review'].tolist()

y=df['sentiment']
y

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

"""##Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer=TfidfVectorizer(max_features=5000)

X_train=vectorizer.fit_transform(X_train).toarray()

X_train.shape

X_test=vectorizer.transform(X_test).toarray()

X_test.shape



"""##Evaluating Classification Models"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from xgboost import XGBClassifier

mnc = MultinomialNB()
knn = KNeighborsClassifier()
svc = SVC()
rfc = RandomForestClassifier(n_estimators=50, random_state=42)
xgbc = XGBClassifier(n_estimators=50, random_state=42)

classifiers = {
    'MultinomialNB': mnc,
    'KNN':knn,
    #'SVC':svc,
    'RandomForestClassifier':rfc,
    'XGBC':xgbc
}

from sklearn.metrics import accuracy_score,precision_score
from sklearn.model_selection import cross_val_score

def train_model(model):
    model.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    score = accuracy_score(y_test, y_pred)
    p_score = precision_score(y_test, y_pred, average='weighted')
    kf_score = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5)
    return str(score*100)+'%' , str(p_score*100)+'%', str(kf_score.mean()*100)+'%'

precision = []
accuracy = []
k_fold = []
start_time = time.time()

for model,classifier in classifiers.items():
    acc,prec,kf = train_model(classifier)
    accuracy.append(acc)
    precision.append(prec)
    k_fold.append(kf)
    finish_time = time.time()
    print(f"{model} has been successfully evaluated. Total time taken is: {(finish_time-start_time)} seconds")
    start_time = finish_time

"""## Comparing Scores"""

score_comparison = pd.DataFrame({'Model': classifiers.keys(), 'Accuracy': accuracy, 'Precision': precision, 'K-Fold': k_fold})
score_comparison

xgb_pred=xgbc.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,xgb_pred))

"""##Prediction on new data"""

def predict_sentiment(review):
  review=[clean_text(review)]
  transformed_text=vectorizer.transform(review).toarray()
  prediction=xgbc.predict(transformed_text)
  if prediction==0:
    return 'Positive'
  elif prediction==1:
    return 'Neutral'
  else:
    return 'Negative'

prediction = predict_sentiment("It is very easy to work with and even the kids love it!")
print(prediction)